{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2f821a-84b0-4b71-bd70-4da01eb9bd73",
   "metadata": {},
   "source": [
    "# Round 1 Initial model evaluation\n",
    "\n",
    "For Round 1 evaluation the outputs of each model are evaluated against the HITL annotated gold-standard dataset:\n",
    "\n",
    "__NER:__\n",
    "\n",
    "- spaCy en_core_web_trf (used as basis for annotation)\n",
    "- flair/ner-english-ontonotes-large\n",
    "\n",
    "__CR:__\n",
    "\n",
    "- fastcoref (used as basis for annotation)\n",
    "- LingMess\n",
    "\n",
    "__REX:__\n",
    "\n",
    "- Babelscape/rebel-large (used as basis for annotation)\n",
    "- Flair (only alternat_name included in annotations)\n",
    "\n",
    "The below code reads in each model's outputs as well as the HITL annotated gold-standard datasets for each task and then compares the results. The main metric used for comparison was the ___Macro F1___ score (in other words the average F1 score across all articles in the corpus). __Precision__ and __Recall__ are also shown where available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d8d85-6885-4392-8e7e-42bcdfe18807",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498a3ce0-bc91-40dd-aa32-781fb3c1f230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from kg_builder import kg\n",
    "from kg_builder import ner\n",
    "from kg_builder import cr\n",
    "from kg_builder import rex\n",
    "from kg_builder import get_wikidata_prepared_info\n",
    "from kg_builder import make_lookup_dict_from_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f59b4-0d43-4882-a733-649ae6826f7a",
   "metadata": {},
   "source": [
    "## Import required data\n",
    "\n",
    "Includes dataframe containing sample data, as well as model outputs and annotations to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fb5a09-3d10-4c74-b327-ad97ebbed246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import original sample data and get a list of the 10 articles designated to 'train'\n",
    "df = pd.read_parquet('source_data/sample_text_30.pq')\n",
    "train_ids = df.loc[df['Split'] == 'train', 'Id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7572d8-a391-4553-bd12-b5c94c2768cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the original json data: in each case the output is a list of Article instances\n",
    "ner_annotations = ner.load_ner_from_label_studio('outputs/annotations/sample_ner_30_annotated.json', df, True)\n",
    "ner_spacy = ner.load_ner_from_label_studio('outputs/round1/sample_ner_30_spacy.json', df, False)\n",
    "ner_flair = ner.load_ner_from_label_studio('outputs/round1/sample_ner_30_flair.json', df, False)\n",
    "\n",
    "cr_annotations = cr.load_cr_from_label_studio('outputs/annotations/sample_cr_30_annotated.json', df, True)\n",
    "cr_fastcoref = cr.load_cr_from_label_studio('outputs/round1/sample_cr_30_fastcoref.json', df, False)\n",
    "cr_lingmess = cr.load_cr_from_label_studio('outputs/round1/sample_cr_30_lingmess.json', df, False)\n",
    "\n",
    "rex_annotations = rex.load_rex_from_label_studio('outputs/annotations/sample_re_30_annotated.json', df, True)\n",
    "rex_rebel = rex.load_rex_from_label_studio('outputs/round1/sample_re_30_rebel.json', df, False)\n",
    "rex_flair = rex.load_rex_from_label_studio('outputs/round1/sample_re_30_flair.json', df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db2606-12e5-4e90-ac61-a33156d86a9e",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c392581-9c0a-4621-8798-1d471b0732c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_task(predictions: list, annotations: list, task: str, selected = []) -> [float, float, float]:\n",
    "    '''\n",
    "    Get precision, recall and F1 for the task (one of 'ner', 'cr', and 'rex')\n",
    "    and print the results.\n",
    "    '''\n",
    "    comparisons = []\n",
    "    if task == 'ner':\n",
    "        for article in predictions:\n",
    "            comparisons.append(ner.calc_article_ner_metrics(article, annotations))\n",
    "        precision, recall, f1 = ner.calc_corpus_ner_metrics(comparisons)  \n",
    "    if task == 'cr':\n",
    "        for article in predictions:\n",
    "            comparisons.append(cr.calc_article_cr_metrics(article, annotations))\n",
    "        precision, recall, f1 = None, None, cr.calc_corpus_cr_metrics(comparisons)  \n",
    "    if task == 'rex':\n",
    "        for article in predictions:\n",
    "            comparisons.append(rex.calc_article_rex_metrics(article, annotations, selected = selected))\n",
    "        precision, recall, f1 = rex.calc_corpus_rex_metrics(comparisons)  \n",
    "    print(f'''precision: {round(precision, 5) if precision is not None else None}\n",
    "recall : {round(recall, 5) if recall is not None else None}\n",
    "macro f1: {round(f1, 5)}''')\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289dd98-65ec-42ef-b96a-f33e444b5d12",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "spaCy performs marginally better than Flair. The metrics for just the test set of 20 articles are also noted below as this will be used to evaluate whether future rounds are improved by any changes or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d62775-ee76-4aca-bb2c-4d9966d2c5b0",
   "metadata": {},
   "source": [
    "#### spaCy full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f77418-b104-4eb2-8a95-9b9cf67bf17e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.91757\n",
      "recall : 0.91592\n",
      "macro f1: 0.91592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9175712334967081, 0.9159155020316009, 0.9159188595938839)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spaCy results\n",
    "evaluate_task(ner_spacy, ner_annotations, task = 'ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f7b9d-37b6-4635-ade7-9eb14a2f6ada",
   "metadata": {},
   "source": [
    "#### Flair full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634cd99b-c20d-48d2-b502-7b0bd31bb61a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.90782\n",
      "recall : 0.91421\n",
      "macro f1: 0.91078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.907819118970955, 0.914214816091885, 0.9107843402218926)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flair results\n",
    "evaluate_task(ner_flair, ner_annotations, task = 'ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0331a-7175-4145-963f-5e7d1232b497",
   "metadata": {},
   "source": [
    "#### spaCy test set results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a58510-190f-47f2-a008-60cbadfb8b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.92177\n",
      "recall : 0.92565\n",
      "macro f1: 0.92278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9217686585090373, 0.925649177622239, 0.9227833400350385)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_spacy_test = [article for article in ner_spacy if article.article_id not in train_ids]\n",
    "ner_annotations_test = [article for article in ner_annotations if article.article_id not in train_ids]\n",
    "evaluate_task(ner_spacy_test, ner_annotations_test, task = 'ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734794b5-bd3a-4424-abdb-6a59565217ae",
   "metadata": {},
   "source": [
    "### CR\n",
    "fastcoref performs better than LingMess. The metrics for just the test set of 20 articles are also noted below as this will be used to evaluate whether future rounds are improved by any changes or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc7fff-8ceb-4a59-b0dc-e2bd1d7a6fec",
   "metadata": {},
   "source": [
    "#### fastcoref full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77fca200-cf36-4739-8f86-e20c71fa726c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: None\n",
      "recall : None\n",
      "macro f1: 0.71688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 0.7168828573284605)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fastcoref results\n",
    "evaluate_task(cr_fastcoref, cr_annotations, task = 'cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be6632-6c2c-469b-8af1-1d92b294183b",
   "metadata": {},
   "source": [
    "#### LingMess full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca2c329-269b-489b-b75e-3a239eb72457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: None\n",
      "recall : None\n",
      "macro f1: 0.68682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 0.6868215051393629)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lingmess results\n",
    "evaluate_task(cr_lingmess, cr_annotations, task = 'cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd35091-8349-4283-a2c2-74ed673d47cb",
   "metadata": {},
   "source": [
    "#### fastcoref test set results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29232077-3334-467f-b3fe-5a4e8c16deb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: None\n",
      "recall : None\n",
      "macro f1: 0.73203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 0.732025492823859)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_fastcoref_test = [article for article in cr_fastcoref if article.article_id not in train_ids]\n",
    "cr_annotations_test = [article for article in cr_annotations if article.article_id not in train_ids]\n",
    "evaluate_task(cr_fastcoref_test, cr_annotations_test, task = 'cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01403b2-6100-46a8-8877-ba917d923be8",
   "metadata": {},
   "source": [
    "### REX\n",
    "\n",
    "To evaluate relation extraction a few preparatory steps are necessary:\n",
    "\n",
    "1) Transform Flair relations to REBEL terminology (except for alternate_name which is unique to Flair)\n",
    "2) Only include those relations pre-identified for inclusion (those that were deemed useful for the KG after review)\n",
    "3) Populate inverse relations (where possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7265bf-edba-416b-9e39-e68fc319fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rebel_flair_overview contains a summary of relations to be included\n",
    "rebel_flair_overview, _, _, _, _, = get_wikidata_prepared_info('reference_info/wikidata_references.pkl')\n",
    "\n",
    "# We only want to evaluate relations which have been preselected for inclusion\n",
    "included_relations = list(rebel_flair_overview.loc[rebel_flair_overview['rebel description'].notna(), 'rebel description'])\n",
    "included_relations += list(make_lookup_dict_from_df(rebel_flair_overview[rebel_flair_overview['rebel description'].notna()], 'rebel description', 'inverse description').values())\n",
    "included_relations += ['alternate_name'] # additional Flair relation not in REBEL\n",
    "included_relations = list(set(included_relations))\n",
    "\n",
    "# To make a like-for-like comparison we want to compare performance of just the \n",
    "# relations shared by the models\n",
    "shared_relations = list(rebel_flair_overview.loc[rebel_flair_overview['wikidata description mapping'].notna(), 'wikidata description mapping'])\n",
    "shared_relations += list(make_lookup_dict_from_df(rebel_flair_overview[rebel_flair_overview['wikidata description mapping'].notna()], 'wikidata description mapping', 'inverse description').values())\n",
    "shared_relations = list(set(shared_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e02428d-4bf4-41d3-9070-55a9d775c8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform flair relations to rebel terminology\n",
    "for article in rex_flair:\n",
    "    rex.flair_to_rebel(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba0f9de7-79f0-4235-a4d7-a4dc13de5d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include pre-identified relations\n",
    "for article in rex_flair:\n",
    "    article.relations = [relation for relation in article.relations if relation.relation_type in included_relations]\n",
    "for article in rex_rebel:\n",
    "    article.relations = [relation for relation in article.relations if relation.relation_type in included_relations]\n",
    "for article in rex_annotations:\n",
    "    article.relations = [relation for relation in article.relations if relation.relation_type in included_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800ea65b-3204-4943-a6ac-663f7466a8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate inverse relations\n",
    "for article in rex_flair:\n",
    "    rex.populate_inverse_relations(article)\n",
    "for article in rex_rebel:\n",
    "    rex.populate_inverse_relations(article)\n",
    "for article in rex_annotations:\n",
    "    rex.populate_inverse_relations(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777c21c-9863-42c9-b569-270a5085c192",
   "metadata": {},
   "source": [
    "#### Flair vs Rebel shared results\n",
    "\n",
    "Results for those relations shared by Flair and Rebel - this is the fairest test of performance since Rebel includes many more relations that Flair does inherently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7adc9ab3-0496-470f-a831-6ae330804022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.37202\n",
      "recall : 0.1825\n",
      "macro f1: 0.22977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37201890701890694, 0.18250212271763996, 0.2297716468646836)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flair shared results\n",
    "evaluate_task(rex_flair, rex_annotations, task = 'rex', selected = shared_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fec96ec-4ec0-401b-98fe-8cf7a38db96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.67765\n",
      "recall : 0.41647\n",
      "macro f1: 0.49206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6776521735345263, 0.4164660132332546, 0.4920582906818482)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REBEL shared results\n",
    "evaluate_task(rex_rebel, rex_annotations, task = 'rex', selected = shared_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e380c-ca2d-4c31-9a92-b67c4be1f0bc",
   "metadata": {},
   "source": [
    "#### Flair vs Rebel overall results\n",
    "\n",
    "Results when comparing all relations against the annotated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14eaf6d1-f5f4-4950-b685-7c9e2044202d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.34059\n",
      "recall : 0.14962\n",
      "macro f1: 0.19685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3405930893961585, 0.14962041768646236, 0.1968470159427254)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flair results overall\n",
    "evaluate_task(rex_flair, rex_annotations, task = 'rex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ada7cb9-3cc2-4c29-bae4-59f22f2339f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.64396\n",
      "recall : 0.49399\n",
      "macro f1: 0.53658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6439566173690335, 0.4939900539374412, 0.5365831581871705)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REBEL results overall\n",
    "evaluate_task(rex_rebel, rex_annotations, task = 'rex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f88a1b-60c9-4f05-be1d-5421ab879685",
   "metadata": {},
   "source": [
    "#### Rebel test results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8f80b8-47cc-4838-8b82-ebd706bed701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.65823\n",
      "recall : 0.4664\n",
      "macro f1: 0.5255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.658230319617822, 0.46639740004529884, 0.525495109705935)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rex_rebel_test = [article for article in rex_rebel if article.article_id not in train_ids]\n",
    "rex_annotations_test = [article for article in rex_annotations if article.article_id not in train_ids]\n",
    "evaluate_task(rex_rebel_test, rex_annotations_test, task = 'rex')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
