{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b832719-b52c-4205-ab6d-c9bcbc012435",
   "metadata": {},
   "source": [
    "# Round 2 Model evaluation\n",
    "\n",
    "For Round 2 just the test set results are evaluated against the original HITL gold-standard annotations (since the train articles were used to investigate and develop improvements)\n",
    "\n",
    "__NER:__\n",
    "\n",
    "- spaCy en_core_web_trf\n",
    "_ + improvements\n",
    "\n",
    "__CR:__\n",
    "\n",
    "- fastcoref\n",
    "- + improvements\n",
    "\n",
    "__REX:__\n",
    "\n",
    "- Babelscape/rebel-large\n",
    "- + alternate_name from Flair\n",
    "\n",
    "The below code reads in each model's outputs as well as the HITL annotated gold-standard datasets for each task and then compares the results. The main metric used for comparison was the ___Macro F1___ score (in other words the average F1 score across all articles in the corpus). __Precision__ and __Recall__ are also shown where available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c8f3b-d9fa-4f54-8e76-d41335f4df6d",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7682999-d9e7-4947-b07a-6164471e770c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from kg_builder import kg\n",
    "from kg_builder import ner\n",
    "from kg_builder import cr\n",
    "from kg_builder import rex\n",
    "from kg_builder import get_wikidata_prepared_info\n",
    "from kg_builder import make_lookup_dict_from_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5eb45f-40a7-4afd-9af1-18f6a6d7b808",
   "metadata": {},
   "source": [
    "## Import required data\n",
    "\n",
    "Includes dataframe containing sample data, as well as model outputs and annotations to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f82c62-5b7d-4269-b5f8-fe30b670fc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import sample data\n",
    "df = pd.read_parquet('source_data/sample_text_30.pq')\n",
    "train_ids = df.loc[df['Split'] == 'train', 'Id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3cb5d3-3c9d-4a19-bf77-f16bdd683b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In each case the output is a list of Article instances\n",
    "ner_annotations = ner.load_ner_from_label_studio('model_outputs/annotations/sample_ner_30_annotated.json', df, True)\n",
    "ner_annotations_test = [article for article in ner_annotations if article.article_id not in train_ids]\n",
    "cr_annotations = cr.load_cr_from_label_studio('model_outputs/annotations/sample_cr_30_annotated.json', df, True)\n",
    "cr_annotations_test = [article for article in cr_annotations if article.article_id not in train_ids]\n",
    "rex_annotations = rex.load_rex_from_label_studio('model_outputs/annotations/sample_re_30_annotated.json', df, True)\n",
    "rex_annotations_test = [article for article in rex_annotations if article.article_id not in train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c241b03-8f7d-4794-a5d0-a350cf89056c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('model_outputs/round2/results.pkl', 'rb') as file:\n",
    "    articles = pickle.load(file)\n",
    "articles_test = [article for article in articles if article.article_id not in train_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef681bb-781a-476e-ad78-b2d4bfa6caee",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0f2b5a-fd26-4d38-9fd5-2db2f9d55b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_task(predictions: list, annotations: list, task: str, selected = []) -> [float, float, float]:\n",
    "    '''\n",
    "    Get precision, recall and F1 for the task (one of 'ner', 'cr', and 'rex')\n",
    "    and print the results.\n",
    "    '''\n",
    "    comparisons = []\n",
    "    if task == 'ner':\n",
    "        for article in predictions:\n",
    "            comparisons.append(ner.calc_article_ner_metrics(article, annotations))\n",
    "        precision, recall, f1 = ner.calc_corpus_ner_metrics(comparisons)  \n",
    "    if task == 'cr':\n",
    "        for article in predictions:\n",
    "            comparisons.append(cr.calc_article_cr_metrics(article, annotations))\n",
    "        precision, recall, f1 = None, None, cr.calc_corpus_cr_metrics(comparisons)  \n",
    "    if task == 'rex':\n",
    "        for article in predictions:\n",
    "            comparisons.append(rex.calc_article_rex_metrics(article, annotations, selected = selected))\n",
    "        precision, recall, f1 = rex.calc_corpus_rex_metrics(comparisons)  \n",
    "    print(f'''precision: {round(precision, 5) if precision is not None else None}\n",
    "recall : {round(recall, 5) if recall is not None else None}\n",
    "macro f1: {round(f1, 5)}''')\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f10623-7a7f-44ae-91c6-16819726f8c6",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46473ec1-33c5-4e20-becb-cd4beebedf04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.93791\n",
      "recall : 0.93795\n",
      "macro f1: 0.93697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9379090717443128, 0.9379529940502028, 0.9369658106230876)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spaCy results\n",
    "evaluate_task(articles_test, ner_annotations_test, task = 'ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6943c-6c58-4f18-a74a-e71adcab4a16",
   "metadata": {},
   "source": [
    "### CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e020af4f-c3d8-4ebe-8e98-c8a47e93d1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: None\n",
      "recall : None\n",
      "macro f1: 0.87199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 0.8719948523074941)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fastcoref results\n",
    "evaluate_task(articles_test, cr_annotations_test, task = 'cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce05882-9b67-4404-b044-afb9f89cf5ab",
   "metadata": {},
   "source": [
    "### REX\n",
    "\n",
    "To evaluate relation extraction a few preparatory steps are necessary:\n",
    "\n",
    "1) Only include those relations pre-identified for inclusion\n",
    "2) Populate inverse relations (where possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c162548-4a88-438c-afa5-ed5c8818e458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rebel_flair_overview contains a summary of relations to be included\n",
    "rebel_flair_overview, _, _, _, _, = get_wikidata_prepared_info('reference_info/wikidata_references.pkl')\n",
    "\n",
    "# We only want to evaluate relations which have been preselected for inclusion\n",
    "included_relations = list(rebel_flair_overview.loc[rebel_flair_overview['rebel description'].notna(), 'rebel description'])\n",
    "included_relations += list(make_lookup_dict_from_df(rebel_flair_overview[rebel_flair_overview['rebel description'].notna()], 'rebel description', 'inverse description').values())\n",
    "included_relations += ['alternate_name'] # additional Flair relation not in REBEL\n",
    "included_relations = list(set(included_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae87e2f2-097a-4507-906b-8aaccc257679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include pre-identified relations\n",
    "for article in articles_test:\n",
    "    article.relations = [relation for relation in article.relations if relation.relation_type in included_relations]\n",
    "for article in rex_annotations_test:\n",
    "    article.relations = [relation for relation in article.relations if relation.relation_type in included_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f4b971-552b-4edd-b381-d62dbb716bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate inverse relations\n",
    "for article in articles_test:\n",
    "    rex.populate_inverse_relations(article)\n",
    "for article in rex_annotations_test:\n",
    "    rex.populate_inverse_relations(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a261337-8f7b-473f-a613-d387500310f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.65184\n",
      "recall : 0.48132\n",
      "macro f1: 0.53236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6518435211463601, 0.48131530377899195, 0.5323606594181386)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REBEL results overall\n",
    "evaluate_task(articles_test, rex_annotations_test, task = 'rex')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
